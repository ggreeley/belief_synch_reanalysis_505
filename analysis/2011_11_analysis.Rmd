---
title: 'Analysis: Revisiting Vlasceanu et al. (2020)'
author: "Garrett D. Greeley"
date: "Document Last Generated: `r format(Sys.time(), '%A, %B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

This document/code accomplishes several tasks. In order, it:

1) Step
2) Step
3) Step
4) Step

If the repo is cloned and the file knits properly, the data from data/analysis is read, visualized, and analyzed.

# Libraries

Libraries that may be needed for analysis, visualization, and additional processing:

```{r}
# modeling
library(lme4)
library(betareg)
library(brms)

# aesthetic
library(showtext)
# https://www.fontsquirrel.com/fonts/latin-modern-roman
font_add(family = "lmroman",
         regular = "/Library/Fonts/lmroman10-regular-webfont.ttf")
font_add(family = "lmroman_b",
         regular = "/Library/Fonts/lmroman10-bold-webfont.ttf")
showtext_auto()

# general
library(tidyverse)
```

# Functions

Helper functions:

```{r}
source(
  here::here("processing", "functions.R")
  )
```

# Data

Read the wrangled data generated by 2021-11_processing.Rmd (see that file for creation and notes):

```{r}
beliefs <- read_csv(
  here::here("data", "analysis", "2021-11_long_beliefs.csv")
  )
rschange <- read_csv(
  here::here("data", "analysis", "2021-11_long_rschange.csv")
  )
synch <- read_csv(
  here::here("data", "analysis", "2021-11_long_synch.csv")
  )
```

# Basic Descriptives

First, let's clean up the participant codes a bit. In the original study, codes were generated by asking participants to provide a combination of their birthplace and birthday - the components were broad enough as to (probably) not be identifiable (e.g., city and month), but I'd prefer something a little more standardized/aesthetic:

```{r participant codes}
beliefs <- beliefs %>%
  mutate(participant_code = participant_code %>%
           as.factor() %>%
           forcats::fct_anon(prefix = "p0"))
```

What is the $N$ (should match manuscript; 168)?:

```{r sample size}
# N = 168 (matches)
beliefs %>%
  group_by(participant_code) %>%
  slice_head(n = 1) %>%
  nrow()

# shorter way, but perhaps less explicit/safe
n_distinct(beliefs$participant_code)
```

Other demographics:

```{r}
indiv_dat <- beliefs %>%
  group_by(participant_code) %>%
  slice_head(n = 1) %>%
  ungroup()

indiv_dat %>%
  count(what_is_your_gender) %>%
  mutate(perc = n / sum(n))

indiv_dat %>%
  mutate(what_is_your_native_language = str_to_lower(what_is_your_native_language)) %>%
  count(what_is_your_native_language) %>%
  mutate(perc = n / sum(n)) %>%
  arrange(-n) %>%
  view()

indiv_dat %>%
  summarise(mean_age = mean(how_old_are_you),
            sd_age = sd(how_old_are_you),
            min_age = min(how_old_are_you),
            max_age = max(how_old_are_you))

indiv_dat %>%
  summarise(mean_politics = mean(political_views),
            sd_politics = sd(political_views),
            min_politics = min(political_views),
            max_politics = max(political_views))
```

Demographic combinations. Political distribution pretty equal across sexes, age, sex/age.

```{r}
indiv_dat %>%
  group_by(what_is_your_gender, political_views) %>%
  count() %>%
  ungroup() %>%
  ggplot(aes(x = political_views, y = n, fill = what_is_your_gender)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = 1:7) +
  theme_light()

indiv_dat %>%
  ggplot(aes(x = political_views, y = how_old_are_you)) +
  geom_jitter() +
  facet_wrap(~ what_is_your_gender) +
  scale_x_continuous(breaks = 1:7) +
  theme_light()
```

# Accuracy & Scientific Support Ratings

## Visualizations

Participant level ratings for each item by rating type (scientific support or accuracy):

```{r}
# rating theme
raw_rating_theme <- function(...) {
  theme_light() +
  theme(plot.title = element_text(family = "lmroman", size = 24, margin = margin(0,0,30,0)),
        title = element_text(family = "lmroman", size = 20),
        axis.text.x = element_text(family = "lmroman", size = 12),
        axis.text.y = element_text(family = "lmroman", size = 12),
        legend.text = element_text(family = "lmroman", size = 14),
        strip.text = element_text(family = "lmroman", color = "black", size = 16),
        strip.background = element_rect(fill = "white", color = "black"))
}

raw_rating_plot_a <- beliefs %>%
  select(participant_code, phase, rating_type, item_category, item_code, raw_rating) %>%
  filter(phase == "p1") %>%
  mutate(rating_type = str_to_title(rating_type),
         raw_rating = as.character(raw_rating),
         raw_rating = case_when(raw_rating == 1 ~ "1 (not at all/definitely not)",
                                raw_rating == 7 ~ "7 (very much so/definitely yes)",
                                TRUE ~ raw_rating)) %>%
  ggplot(aes(x = item_code, y = participant_code, fill = raw_rating)) +
  geom_tile() +
  scale_fill_viridis_d(option = "magma") +
  labs(title = "A) Phase #1 Ratings",
       x = "Statement Code",
       y = "Participant (One Per Row)",
       fill = "Raw Rating") +
  scale_y_discrete(breaks = c("p0001", "p0084", "p0168")) +
  facet_wrap(~ rating_type, scales = "free") +
  raw_rating_theme()

raw_rating_plot_b <- beliefs %>%
  select(participant_code, phase, rating_type, item_category, item_code, raw_rating) %>%
  filter(phase == "p2") %>%
  mutate(rating_type = str_to_title(rating_type),
         raw_rating = as.character(raw_rating),
         raw_rating = case_when(raw_rating == 1 ~ "1 (not at all/definitely not)",
                                raw_rating == 7 ~ "7 (very much so/definitely yes)",
                                TRUE ~ raw_rating)) %>%
  ggplot(aes(x = item_code, y = participant_code, fill = raw_rating)) +
  geom_tile() +
  scale_fill_viridis_d(option = "magma") +
  labs(title = "B) Phase #2 Ratings",
       x = "Statement Code",
       y = "Participant (One Per Row)",
       fill = "Raw Rating") +
  scale_y_discrete(breaks = c("p0001", "p0084", "p0168")) +
  facet_wrap(~ rating_type, scales = "free") +
  raw_rating_theme()

raw_rating_plot_a
raw_rating_plot_b
```

Save:

```{r save heatmaps, include=FALSE}
#ggsave(filename = here::here(
#  "figs",
#  "figs_final",
#  paste0(format(Sys.time(), "%Y-%m-%d_%H-%M_%S_"), "raw_rating_plot_a", ".png")
#  ),
#  plot = raw_rating_plot_a,
#  width = 14)

#ggsave(filename = here::here(
#  "figs",
#  "figs_final",
#  paste0(format(Sys.time(), "%Y-%m-%d_%H-%M_%S_"), "raw_rating_plot_b", ".png")
#  ),
#  plot = raw_rating_plot_b,
#  width = 14)
```

Participant level ratings for each item by rating type **and** category (N, A, H, V; meaning unknown). Can you spot the myths?:

```{r}
beliefs %>%
  select(participant_code, phase, rating_type, item_category, item_code, raw_rating) %>%
  mutate(rating_type = str_to_title(rating_type),
         raw_rating = as.character(raw_rating),
         raw_rating = case_when(raw_rating == 1 ~ "1 (not at all/definitely not)",
                                raw_rating == 7 ~ "7 (very much so/definitely yes)",
                                TRUE ~ raw_rating)) %>%
  ggplot(aes(x = item_code, y = participant_code, fill = raw_rating)) +
  geom_tile() +
  scale_fill_viridis_d(option = "magma") +
  labs(title = "B) Phase #2 Ratings",
       x = "Statement Code",
       y = "Participant (One Per Row)",
       fill = "Raw Rating") +
  scale_y_discrete(breaks = c("p0001", "p0084", "p0168")) +
  facet_wrap(~ rating_type + item_category, scales = "free", ncol = 2) +
  raw_rating_theme()
```

## Phase Correlations

```{r}
belief_cor <- beliefs %>%
  select(participant_code, condition, rating_type, item_category, item_code, phase, raw_rating) %>%
  pivot_wider(names_from = phase,
              values_from = raw_rating)

indiv_accuracy_phase_cor <- belief_cor %>%
  filter(rating_type == "accuracy") %>%
  group_by(participant_code) %>%
  group_split() %>%
  map(~ broom::tidy(cor.test(.$p1, .$p2))) %>%
  bind_rows()
  
indiv_science_phase_cor <- belief_cor %>%
  filter(rating_type == "science") %>%
  group_by(participant_code) %>%
  group_split() %>%
  map(~ broom::tidy(cor.test(.$p1, .$p2))) %>%
  bind_rows()
```

Plot:

```{r}

indiv_accuracy_phase_cor %>%
  mutate(tmp_particpant = row_number(),
         var_cor = "Accuracy of Statement") %>%
  bind_rows(.,
            indiv_science_phase_cor %>%
              mutate(tmp_particpant = row_number(),
                     var_cor = "Scientific Support for Statement")) %>%
  group_by(var_cor) %>%
  mutate(mean_cor = mean(estimate, na.rm = TRUE)) %>%
  ggplot(aes(x = tmp_particpant, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low,
                ymax = conf.high)) +
  facet_wrap(~ var_cor) +
  geom_hline(aes(yintercept = mean_cor),
             color = "red") +
  labs(x = "Participant",
       y = "Pearson Correlation (r)") +
  raw_rating_theme()
  #ggthemes::theme_economist()
    
```

## Analyses (#1)

First, missing data?

```{r}
belief_cor %>%
  filter(is.environment(p1)) %>%
  nrow()

belief_cor %>%
  filter(is.environment(p2)) %>%
  nrow()
```

No. Impressive given 168 participants and 48 total ratings per phase (e.g., $168_{participants} \times 2_{phases} \times 24_{questions/phase} \times 2_{ratings/question} = 16128$ data points), but it was a laboratory study so reasonable (participants may not have been able to advance to subsequent questions without providing ratings).

The plots above suggest generally increasing believability across the two phases. 

Prep data for analyses. First, merge in myth/fact information to the belief_cor data and mutate a "change" indicator column for mixed-effect logistic regression. Second, trim down "belief" data to only columns that should be included for modeling (mixed-effect linear regression; possibly mixed-effect multinomial logistic regression [1-7 scale is ordinal, but 7 levels is *normally* treated as continuous - at least it's not 3/4/5! - inclusion of this model depends on my motivation over the next few days])

```{r}
belief_logit_dat <- belief_cor %>%
  left_join(.,
            beliefs %>%
              select(participant_code, condition, item_category,
                     rating_type, political_views, fact_or_myth, item_code),
            by = c("participant_code", "condition", "item_category", "item_code", "rating_type")) %>%
  # unclear why rows doubling despite very specific merge criteria
  # for now, solve by only keeping one of each doubled row
  group_by(participant_code, rating_type, item_category, item_code) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  mutate(rating_diff = p2 - p1,
         rating_change = ifelse(rating_diff == 0, 0, 1),
         rating_direction = ifelse(rating_diff > 0, "increase",
                                   ifelse(rating_diff < 0, "decrease",
                                          "same")
                                   ),
         condition = as.factor(condition))

belief_multinom_dat <- beliefs %>%
  select(participant_code, condition, item_category,
         rating_type, political_views, fact_or_myth,
         phase, item_code, raw_rating) %>%
  # in case participant slopes desired on phase
  mutate(phase_num = parse_number(phase) - 1)
```

### Mixed Effect Logistic Regression (Frequentist)

Empty/Null model to assess participant and item intercept variability.

Note that this random effect structure is *crossed*: all participants provided multiple ratings on all items.

```{r}
belief_logit_dat %>%
  count(rating_change) # no excessive 1s/0s
```

```{r}
belief_change_null <- glmer(rating_change ~ 1 + (1 | participant_code) + (1 | item_code),
                            family = binomial(link = "logit"),
                            data = belief_logit_dat)

summary(belief_change_null)
```

ICC - note that this ICC represents the variance in belief change (1/0) that is between participants **and** items. Thus, about 10.5% of the variance in belief change is between participants (person-to-person variability) and between items (item-to-item variability). 

```{r}
# store variances from each random effect
belief_change_null_part_var <- 0.28736
belief_change_null_item_var <- 0.09833

# calculate manually (performance::icc() only gives 1 value, unclear how it's derived)
# see ./analysis/functions.R
# between participants
icc_logit(rand_effect_var = belief_change_null_part_var)
# between items
icc_logit(rand_effect_var = belief_change_null_item_var)

# combining random effect variances
(belief_change_null_part_var + belief_change_null_item_var) /
  ((belief_change_null_part_var + belief_change_null_item_var) + (pi^2 / 3))

# this matches random effect combo
performance::icc(belief_change_null)
```

Separating these (in the code chunk above), it appears the largest chunk of variance (other than *within-person* variance) is between participants. See `belief_change_null_part_var / (belief_change_null_part_var + (pi^2 / 3))`, which gives 0.08 (~8% of variability in belief change is between participants; ~3% is between items).

This has little bearing on the conditional models that follow. This mostly just demonstrates that a mixed model is indeed called for here, given the variance in the outcome absorbed by the random effects.

Now fit conditional models. The first includes the fixed effects central to the study (condition, rating type, and fact/myth status of the item). The second includes additional covariates.

```{r}
# conditional model w/ central predictors
belief_change_cond_1 <- glmer(rating_change ~ 1 +
                              condition +
                              rating_type +
                              fact_or_myth +
                              (1 | participant_code) + (1 | item_code),
                            family = binomial(link = "logit"),
                            data = belief_logit_dat)

summary(belief_change_cond_1)

# adding covariates
belief_change_cond_2 <- glmer(rating_change ~ 1 +
                              condition +
                              rating_type +
                              fact_or_myth +
                              item_category +
                              political_views +
                              (1 | participant_code) + (1 | item_code),
                            family = binomial(link = "logit"),
                            data = belief_logit_dat)

summary(belief_change_cond_2)
```

Inclusion of covariates (item category and political views) has an interesting effect. The "political views" fixed effect is statistically significant, default category contrasts (each vs. "a") are not, and AIC and BIC increased.

Perhaps most interestingly, there is not much of a difference in belief change between conditions (not statistically significant, at least). But there **is** a difference in belief change as a function of "rating type" - that is, accuracy vs. scientific support ratings. 

```{r}
broom.mixed::tidy(belief_change_cond_2) %>%
  mutate(estimate_oddratio = ifelse(effect == "fixed", exp(estimate), NA),
         .after = estimate) %>%
  select(-statistic)

#stargazer::stargazer(belief_change_cond_2,
#                     type = "latex",
#                     out = here::here("figs", "figs_temp", "belief_change_cond_2_tab.tex"))
```

Predict:

```{r}
belief_logit_dat %>%
  mutate(full_mod_predict_log = predict(belief_change_cond_2),
         full_mod_predict_log_norandef = predict(belief_change_cond_2,
                                                 re.form = NA),
         full_mod_predict_prob = predict(belief_change_cond_2, type = "response"),
         full_mod_predict_prob_norandef = predict(belief_change_cond_2, type = "response",
                                                  re.form = NA))
```

Plots. A lot of code repetition here, could modularize later:

```{r}
# belief change probability as a function of condition (ns)
belief_change_plot_a <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ condition,
                 type = "response") %>%
  data.frame() %>%
  ggplot(aes(x = condition, y = prob, fill = condition)) +
  geom_bar(stat = "identity",
           alpha = 0.8) +
  geom_errorbar(aes(ymin = asymp.LCL,
                    ymax = asymp.UCL),
                width = 0.25,
                color = "black") +
  scale_fill_manual(values = c("darkred", "#000080")) +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "A) Condition",
       fill = "Condition",
       x = "Condition",
       y = "Rating Change Probability") +
  raw_rating_theme()

# belief change probability as a function of item being fact/myth (sig)
belief_change_plot_b <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ fact_or_myth,
                 type = "response") %>%
  data.frame() %>%
  mutate(fact_or_myth = str_to_title(fact_or_myth)) %>%
  ggplot(aes(x = fact_or_myth, y = prob)) +
  geom_bar(stat = "identity",
           fill = "#134e13",
           alpha = 0.8) +
  geom_errorbar(aes(ymin = asymp.LCL,
                    ymax = asymp.UCL),
                width = 0.25,
                color = "black") +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "B) Fact vs. Myth",
       x = "Fact vs. Myth",
       y = "Rating Change Probability") +
  raw_rating_theme()

# belief change probability as a function of rating type (sig)
belief_change_plot_c <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ rating_type,
                 type = "response") %>%
  data.frame() %>%
  mutate(rating_type = str_to_title(rating_type)) %>%
  ggplot(aes(x = rating_type, y = prob)) +
  geom_bar(stat = "identity",
           fill = "#134e13",
           alpha = 0.8) +
  geom_errorbar(aes(ymin = asymp.LCL,
                    ymax = asymp.UCL),
                width = 0.25,
                color = "black") +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "C) Rating Type",
       x = "Rating Type",
       y = "Rating Change Probability") +
  raw_rating_theme()

# belief change probability as a function of item category
belief_change_plot_d <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ item_category,
                 type = "response") %>%
  data.frame() %>%
  ggplot(aes(x = item_category, y = prob)) +
  geom_bar(stat = "identity",
           fill = "#134e13",
           alpha = 0.8) +
  geom_errorbar(aes(ymin = asymp.LCL,
                    ymax = asymp.UCL),
                width = 0.25,
                color = "black") +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "D) Item Category",
       x = "Item Category",
       y = "Rating Change Probability") +
  raw_rating_theme()

# belief change probability as a function of political views
belief_change_plot_e <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ political_views,
                 at = list(political_views = 1:7),
                 type = "response") %>%
  data.frame() %>%
  ggplot(aes(x = political_views, y = prob)) +
  geom_ribbon(aes(ymin = asymp.LCL,
                ymax = asymp.UCL),
              fill = "lightgray",
              alpha = 0.30) +
  geom_line(size = 2,
            color = "#134e13") +
  scale_x_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "E) Political Views",
       x = "Political Views",
       y = "Rating Change Probability") +
  raw_rating_theme()

# combination - while no interaction explored, nice to see in one place
belief_change_plot_f <- emmeans::emmeans(belief_change_cond_2,
                 specs = ~ political_views + condition + fact_or_myth,
                 at = list(political_views = 1:7),
                 type = "response") %>%
  data.frame() %>% 
  mutate(fact_or_myth = str_to_title(fact_or_myth)) %>%
  ggplot(aes(x = political_views, y = prob, color = condition)) +
  geom_ribbon(aes(ymin = asymp.LCL,
                ymax = asymp.UCL),
              fill = "lightgray",
              alpha = 0.30) +
  geom_line(size = 2) +
  scale_color_manual(values = c("darkred", "#000080")) +
  facet_wrap(~ fact_or_myth + condition, nrow = 1) +
  coord_cartesian(ylim = c(0.3, 0.8)) +
  labs(title = "F) Combo Plot",
       color = "Condition",
       x = "Political Views",
       y = "Rating Change Probability") +
  raw_rating_theme()
```

View / construct plots. Proud of this one:

```{r}
belief_change_abcdef <- 
  (belief_change_plot_a / belief_change_plot_b / belief_change_plot_c) |
  (belief_change_plot_d / belief_change_plot_e / belief_change_plot_f) 

belief_change_abcdef

#ggsave(filename = here::here(
#  "figs",
#  "figs_final",
#  paste0(format(Sys.time(), "%Y-%m-%d_%H-%M_%S_"), "logit_mod_population_effects", ".png")
#  ),
#  plot = belief_change_abcdef,
#  width = 14,
#  height = 12)
```

### Mixed Effect Logistic Regression (Bayesian)

```{r}
belief_change_cond_2_brm <- brm(rating_change ~ 1 +
      condition +
      rating_type +
      fact_or_myth +
      item_category +
      political_views +
      (1 | participant_code) + (1 | item_code),
    family = binomial(link = "logit"),
    data = belief_logit_dat,
    file = "analysis/brms_fits/belief_change_cond_2_brm")

summary(belief_change_cond_2_brm)
plot(belief_change_cond_2_brm)
```

## Analysis (#2)

### Mixed Effect Ordinal Logistic Regression (Frequentist)

So, 7 levels is wild to start with for my *first* mixed effect ordinal regression. Thus, sticking with same logit data as before but using "rating_change" as outcome, with 3 ordinal levels: decrease, same, increase.

```{r}
belief_logit_dat %>%
  count(rating_direction) # no excessive levels

# for ordinal::clmm, rating_direction needs to be a factor
belief_logit_dat$rating_direction <- factor(belief_logit_dat$rating_direction,
                                               levels = c("decrease", "same", "increase"))

levels(belief_logit_dat$rating_direction)
```

Empty/Null model:

```{r}
belief_ord_null <- ordinal::clmm(rating_direction ~ 1 + (1 | participant_code) + (1 | item_code),
              link = "logit", # proportional odds
              data = belief_logit_dat)

summary(belief_ord_null)
```

Conditional models:

```{r}
belief_ord_cond_1 <- ordinal::clmm(rating_direction ~ 1 +
                                     condition +
                                     rating_type +
                                     fact_or_myth +
                                     (1 | participant_code) + (1 | item_code),
              link = "logit", # proportional odds
              data = belief_logit_dat)

summary(belief_ord_cond_1)

belief_ord_cond_2 <- ordinal::clmm(rating_direction ~ 1 +
                                     condition +
                                     rating_type +
                                     fact_or_myth +
                                     item_category +
                                     political_views +
                                     (1 | participant_code) + (1 | item_code),
              link = "logit", # proportional odds
              data = belief_logit_dat)

summary(belief_ord_cond_2)
```

Odds ratios:

```{r}
exp(belief_ord_cond_2$coefficients)
```

As for table, have to make manually since `clmm::clmm()` is not a supported model object type.

Get probability estimates and or relative probabilities. Could not find a way to get predicted ordinal category probabilities as a function of predictors with model object + emmeans. `brms::conditional_effects` makes this easy. The commented out codes represent some efforts. Note that the first few attempts give (what I think are) the probabilities of being in a higher category (e.g., same or increased rating, relative to decreasing).:

```{r}
emmeans::emmeans(belief_ord_cond_2,
                 specs = ~ condition,
                 type = "response")

emmeans::emmeans(belief_ord_cond_2,
                 specs = ~ rating_type,
                 type = "response")

emmeans::emmeans(belief_ord_cond_2,
                 specs = ~ political_views,
                 at = list(political_views = 1:7),
                 type = "response")

emmeans::emmeans(belief_ord_cond_2,
                 specs = ~ political_views,
                 at = list(political_views = 1:7),
                 type = "response")
#
#tibble(participant_code = NA,
#       item_code = NA,
#       political_views = rep(1:7, times = 2),
#       fact_or_myth = c(rep("fact", times = 7), rep("myth", times = 7)),
#       condition = c(rep(1, times = 7), rep(2, times = 7)))
#
#belief_logit_dat %>%
#  select(item_category,
#         political_views, fact_or_myth, condition, rating_type,
#         rating_direction) %>%
#  mutate(participant_code = NA,
#         item_code = NA,
#         ord_cond_2_pred_prop_noranef = predict.glm(belief_ord_cond_2,
#                                                    type = "response")) %>%
#  group_by(item_category,
#         political_views, fact_or_myth, condition, rating_type,
#         rating_direction) %>%
#  slice_head(n = 1) %>%
#  ungroup() %>%
#  group_by(fact_or_myth, rating_direction) %>%
#  summarise(mean_prob = mean(ord_cond_2_pred_prop_noranef))
#
#crossing(political_views = 1:7,
#         fact_or_myth = c("fact", "myth"),
#         condition = 1:2,
#         rating_type = c("accuracy", "science"),
#         item_category = c("a", "h", "n", "v"))
#
#marginal_effects(belief_ord_cond_2, "political_views")
#
#belief_logit_dat %>%
#  mutate(ord_cond_2_pred_prop = predict.glm(belief_ord_cond_2, type = "response"),
#         ord_cond_2_pred_prop_noranef = predict.glm(belief_ord_cond_2,
#                                                    re.form = NA,
#                                                    type = "response")) %>% view()
#  ggplot(aes(x = political_views, y = ord_cond_2_pred_prop_noranef, color = rating_direction)) +
#  geom_line() +
#  scale_x_continuous(breaks = 1:7)  +
#  facet_wrap(~ fact_or_myth + rating_type + condition + item_category)
#
#predict.glm(belief_ord_cond_2,
#        data = belief_logit_dat,
#        type = "response")
#
```

### Mixed Effect Ordinal Logistic Regression (Bayesian)

Aligns with frequentist model across the board.

```{r}
belief_logit_dat <- belief_logit_dat %>%
  mutate(rating_direction_num = case_when(
    rating_direction == "decrease" ~ 1,
    rating_direction == "same" ~ 2,
    rating_direction == "increase" ~ 3,
  ))

belief_ord_cond_2_brm <- brm(rating_direction_num ~ 1 +
                                     condition +
                                     rating_type +
                                     fact_or_myth +
                                     item_category +
                                     political_views +
                                     (1 | participant_code) + (1 | item_code),
                             family = cumulative(link = "logit"),
                             data = belief_logit_dat,
                             file = "analysis/brms_fits/belief_ord_cond_2_brm")

summary(belief_ord_cond_2_brm)

# plot posterior samples
plot(belief_ord_cond_2_brm)
```

Probability estimates of the response levels.

Note that, beyond the chosen x, probabilities are conditional on other variables (covariates) being a) at the mean of the for continuous predictors and b) at the reference group for categorical predictors. That is, if considering the probability of each outcome (decrease, same, increase) as a function of political views, probability estimates are based on holding "condition" at 1, "rating_type" at accuracy, "fact_or_myth" at fact, and "item_category" at a. This can be adjusted with the `condition` argument within `brms::conditional_effects()` - see docs.

I should really functionalize/modularize this at some point. A lot of repitition - something like `brms::conditional_effects()` with these aesthetics mapped onto the data frames in the list would be cool. For another day...

```{r}
#conditional_effects(belief_ord_cond_2_brm, "political_views", categorical = TRUE)
#conditional_effects(belief_ord_cond_2_brm, "condition", categorical = TRUE)
#conditional_effects(belief_ord_cond_2_brm, "rating_type", categorical = TRUE)
#conditional_effects(belief_ord_cond_2_brm, "fact_or_myth", categorical = TRUE)
#conditional_effects(belief_ord_cond_2_brm, "item_category", categorical = TRUE)

# default re_form = NA, so no random effects used (nice for population effects)
belief_ord_cond_2_brm_effects <- conditional_effects(belief_ord_cond_2_brm,
                                                     categorical = TRUE)

# in case wanted
belief_ord_cond_2_brm_effects_wrandom <- conditional_effects(belief_ord_cond_2_brm,
                                                             re_formula = NULL,
                                                             categorical = TRUE)

# colors I like for signling decrease/same/increase
ordinal_colors <- c("darkred", "grey50", "#008000")

# stored data frames are in order from formula 
# condition estimates
belief_direction_plot_a <- belief_ord_cond_2_brm_effects[1] %>%
  data.frame() %>%
  janitor::clean_names() %>%
  mutate(condition_cats_cats = case_when(
    condition_cats_cats == 1 ~ "Decrease",
    condition_cats_cats == 2 ~ "Stay Same",
    condition_cats_cats == 3 ~ "Increase"
  ),
  condition_cats_cats = factor(condition_cats_cats,
                               levels = c("Decrease", "Stay Same", "Increase"))
  ) %>%
  ggplot(aes(x = condition_cats_condition,
             y = condition_cats_estimate,
             fill = condition_cats_cats)) +
  geom_bar(stat = "identity",
           position = "dodge") +
  geom_errorbar(aes(ymin = condition_cats_lower,
                    ymax = condition_cats_upper),
                width = 0.25,
                position = position_dodge(width = 0.90)) +
  scale_fill_manual(values = ordinal_colors) +
  coord_cartesian(ylim = c(0,.60)) +
  labs(title = "A) Condition",
       x = "Condition",
       y = "Probability",
       fill = "Rating Direction") +
  raw_rating_theme()

# rating_type estimates
belief_direction_plot_b <- belief_ord_cond_2_brm_effects[2] %>%
  data.frame() %>%
  janitor::clean_names() %>%
  mutate(rating_type_cats_cats = case_when(
    rating_type_cats_cats == 1 ~ "Decrease",
    rating_type_cats_cats == 2 ~ "Stay Same",
    rating_type_cats_cats == 3 ~ "Increase"
  ),
  rating_type_cats_cats = factor(rating_type_cats_cats,
                               levels = c("Decrease", "Stay Same", "Increase")),
  
  rating_type_cats_rating_type = str_to_title(rating_type_cats_rating_type)) %>%
  ggplot(aes(x = rating_type_cats_rating_type,
             y = rating_type_cats_estimate,
             fill = rating_type_cats_cats)) +
  geom_bar(stat = "identity",
           position = "dodge") +
  geom_errorbar(aes(ymin = rating_type_cats_lower,
                    ymax = rating_type_cats_upper),
                width = 0.25,
                position = position_dodge(width = 0.90)) +
  scale_fill_manual(values = ordinal_colors) +
  coord_cartesian(ylim = c(0,.60)) +
  labs(title = "B) Rating Type",
       x = "Rating Type",
       y = "Probability",
       fill = "Rating Direction") +
  raw_rating_theme()

# fact_or_myth estimates
belief_direction_plot_c <- belief_ord_cond_2_brm_effects[3] %>%
  data.frame() %>%
  janitor::clean_names() %>%
  mutate(fact_or_myth_cats_cats = case_when(
    fact_or_myth_cats_cats == 1 ~ "Decrease",
    fact_or_myth_cats_cats == 2 ~ "Stay Same",
    fact_or_myth_cats_cats == 3 ~ "Increase"
  ),
  fact_or_myth_cats_cats = factor(fact_or_myth_cats_cats,
                               levels = c("Decrease", "Stay Same", "Increase")),
  fact_or_myth_cats_fact_or_myth = str_to_title(fact_or_myth_cats_fact_or_myth)) %>%
  ggplot(aes(x = fact_or_myth_cats_fact_or_myth,
             y = fact_or_myth_cats_estimate,
             fill = fact_or_myth_cats_cats)) +
  geom_bar(stat = "identity",
           position = "dodge") +
  geom_errorbar(aes(ymin = fact_or_myth_cats_lower,
                    ymax = fact_or_myth_cats_upper),
                width = 0.25,
                position = position_dodge(width = 0.90)) +
  scale_fill_manual(values = ordinal_colors) +
  coord_cartesian(ylim = c(0,.60)) +
  labs(title = "C) Fact or Myth",
       x = "Fact or Myth",
       y = "Probability",
       fill = "Rating Direction") +
  raw_rating_theme()

# item_category estimates
#belief_ord_cond_2_brm_effects[4] %>%
#  data.frame() %>%
#  janitor::clean_names() %>%
#  view()

# political_view estimates
belief_direction_plot_d <- belief_ord_cond_2_brm_effects[5] %>%
  data.frame() %>%
  janitor::clean_names() %>%
  mutate(political_views_cats_cats = case_when(
    political_views_cats_cats == 1 ~ "Decrease",
    political_views_cats_cats == 2 ~ "Stay Same",
    political_views_cats_cats == 3 ~ "Increase"
  ),
  political_views_cats_cats = factor(political_views_cats_cats,
                               levels = c("Decrease", "Stay Same", "Increase"))) %>%
  ggplot(aes(x = political_views_cats_political_views,
             y = political_views_cats_estimate,
             color = political_views_cats_cats)) +
  geom_ribbon(aes(ymin = political_views_cats_lower,
                  ymax = political_views_cats_upper,
                  group = political_views_cats_cats),
              fill = "lightgray",
              alpha = 0.30) +
  geom_line(aes(group = political_views_cats_cats),
            size = 2) +
  scale_color_manual(values = ordinal_colors) +
  coord_cartesian(ylim = c(0,.60)) +
  scale_x_continuous(breaks = 1:7) +
  labs(title = "D) Political Views",
       x = "Political Views",
       y = "Probability",
       color = "Rating Direction") +
  raw_rating_theme()
```

View and combine plots:

```{r}
belief_direction_plot_a
belief_direction_plot_b
belief_direction_plot_c
belief_direction_plot_d

(belief_direction_plot_a / belief_direction_plot_b) |
  (belief_direction_plot_c / belief_direction_plot_d)
```



# R/S Missing Data Analysis

What percentage of data is missing at each R/S level / retrieval practice combination (e.g., R/S -4 for practiced items)?

Looks systematic

```{r}
rschange %>%
  filter(retrieval_practice != "all") %>%
  mutate(rs_level = fct_reorder(rs_level, rs_level_num)) %>%
  group_by(retrieval_practice, rs_level) %>%
  summarise(perc_missing = mean(missing_data) * 100) %>%
  ungroup()
```

Prep data for modeling (remove aggregate "all" level for each person):

```{r}
rschange_mod_dat <- rschange %>%
  filter(retrieval_practice != "all") %>%
  # for modeling, set rs_level reference to 0
  # "Beliefs that were unmentioned and unrelated to those mentioned
  # received a score of 0 on the R/S scale."
  mutate(rs_level = factor(rs_level, levels = c("zero",
                                                "neg4",
                                                "neg3",
                                                "neg2",
                                                "neg1",
                                                "pos1",
                                                "pos2",
                                                "pos3",
                                                "pos4")))
```

And analyze via multilevel logistic regression with `lme4::glmer` (frequentist):

```{r}
rschange_missing_glm <- glmer(missing_data ~ retrieval_practice + rs_level + condition + (1 | response_id),
                              family = binomial(link = "logit"), # bernoulli, but special case of binom
                              data = rschange_mod_dat)

summary(rschange_missing_glm)
```

Bayesian version, just for practice:

```{r}
rschange_missing_brm <- brm(missing_data ~ retrieval_practice + rs_level + condition + (1 | response_id),
                            family = bernoulli(link = "logit"), # only 0/1 so more efficient
                            data = rschange_mod_dat)

summary(rschange_missing_brm)
```

